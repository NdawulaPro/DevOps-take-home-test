# DevOps-take-home-test
My Line of Thinking for Solution

1. Deploying Two Independent Containers: To deploy two independent containers, we can create two separate Deployments, each having its own set of replicas, pods, and services. We can also create a horizontal pod autoscaler (HPA) for each Deployment to automatically scale the pods based on CPU utilization. Deploying Two Independent Containers: We can create two Deployments, one for users( view 1.yaml in the take-home-exercise Folder) and one for shifts (view 2.yaml in the Take-home-exercise folder, each with its own set of replicas, pods, and services:

2. Autoscaling the Service: To auto scale the service, we can use the Kubernetes Horizontal Pod Autoscaler (HPA) with the "CPU utilization" metric. The HPA monitors the average CPU utilization of the pods and adjusts the number of replicas based on the target CPU utilization. The HPA can be configured to scale up or down the replicas to maintain the target CPU utilization percentage. We can set the HPA to scale up when the average CPU utilization reaches 70% and scale down when it goes below 50%.kindly view the autoscale1.yaml manifest file for users and autoscale2.yaml for shift users

https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/

3. Rolling Deployments and Rollbacks: To handle rolling deployments and rollbacks, we can use Kubernetes Deployments with Rolling Updates. Rolling Updates allows us to deploy new versions of the application without downtime. When a new version is deployed, Kubernetes replaces the old pods with new ones gradually, ensuring that the service remains available throughout the process. If any issues are detected during the deployment, we can perform a rollback by simply rolling back to the previous version. Rolling Deployments and Rollbacks: We can enable Rolling Updates for the Deployments by adding the following lines below which has been appended to autoscale1.yaml and autoscale2.yaml: spec: strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 https://kubernetes.io/docs/tutorials/kubernetes-basics/update/update-intro/ You can perform a rolling update by running the following command below in your bash terminal :
kubectl set image deployment/users-api users-container=php:7.4-apache-v2 To perform a rollback, we can run the same command below but specify the previous image :

kubectl set image deployment/users-api users-container=php:7.4-apache-v1 What is Kubectl? kubectl, allows you to run commands against Kubernetes cluster. Furthermore, The kubectl set command is used to overwrite or set the given cluster. It allows the user to overwrite the property while working similarly to the kubectl run command

4. IAM(Identity and Access Management) Control: To restrict certain commands from the development team, we can use Kubernetes Role-Based Access Control (RBAC). We can create two roles: one with full access for the admin team and another with limited access for the development team. The limited access role can be configured to allow the team to deploy and roll back, but not to run certain commands.To restrict certain commands on the Kubernetes cluster, we can use RBAC (Role-Based Access Control) and create a Role with the appropriate permissions as seen in restrict.yaml file. Furthermore, This will give the dev-team user permissions to create, update, and delete Deployments, but not other resources such as Pods, Services, or ConfigMaps. https://kubernetes.io/docs/reference/access-authn-authz/rbac/
5. Bonus: To apply the configs to multiple environments (staging vs. production), we can use Kubernetes ConfigMaps and Secrets. We can create separate ConfigMaps and Secrets for each environment and use Kubernetes labels to associate the appropriate resources with each environment. This will allow us to easily manage the configurations for each environment and ensure that the correct configurations are applied to the correct resources. The implementation is found in the config.yaml file To auto-scale the deployment based on network latency instead of CPU, we can use the Kubernetes Horizontal Pod Autoscaler with the "network latency" metric. This metric can be obtained using Kubernetes custom metrics, which can be gathered using Prometheus or another monitoring tool. We can create a custom metric that measures the network latency between the pods and use it to scale up or down the replicas based on the target network latency.kindly view networklatency.yaml
https://cloud.google.com/kubernetes-engine/docs/how-to/horizontal-pod-autoscaling This HPA in networklatency.yaml will scale the users Deployment based on the average network latency between the Pods.

6.To enable network latency metrics, we need to install Kubernetes Metrics Server and create a ServiceMonitor for our application. Below is an example of Installing Metrics Server below : kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

7. Create a ServiceMonitor for users and shifts Deployments is found in servicemonitor.yaml file.
Add latency metric to our application code could be written in python or go language. or the latency metric,a go script was used and it is named addlatency.go file

[Take-home-exercise.zip](https://github.com/NdawulaPro/DevOps-take-home-test/files/10877822/Take-home-exercise.zip)
